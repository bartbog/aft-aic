%% \todo{Some complexity results.
%% Probably: well-founded/KK: polynomially computable (data complexity), stable fixpoints: NP-complete, ... Copy results from logic programming basically. Similarly, complexity increases if we start using ultimate approximators}

In this section, we study complexity of some tasks related to AFT-style semantics for active integrity constraints. 
Before we do so, we provide a new complexity result for logic programs, that we will use later on in our proofs. 

\subsection{A Novel Complexity Result for Logic Programming} 

In this section, we show that deciding whether a \emph{simple} logic program has a grounded fixpoint is $\Sigma^P_2$-complete by establishing a reduction from the problem for arbitrary normal logic programs. 
The intuition is simply to duplicate every atom $a$ as $a^b$ and $a^h$, to be used (only) in bodies or heads of rules, respectively, and add the necessary rules to ensure that they have the same interpretation in any fixpoint of the immediate consequence operator.

We assume the set $\atoms$ to be fixed; for $I\subseteq\atoms$, we define sets $I^b=\{a^b\mid a\in I\}$ and $I^h=\{a^h\mid a\in I\}$.
We extend these notations to literals in the natural way.

\begin{definition}
  Let $\PP$ be a logic program.
  Its simple counterpart is the logic program $\PP^s$ over $\voc^h\cup\voc^b$ containing:
  \begin{itemize}
  %\item for each rule $a\leftarrow l_1,\ldots,l_n$ from $\PP$, the rule $a^h\leftarrow l^b_1,\ldots,l^b_n$;
  \item for each rule $r\in\PP$, the rule $\head(r)^h\leftarrow\body(r)^b$;
  \item rules $a^b\leftarrow a^h$ for each $a\in\voc$.
  \end{itemize}
\end{definition}

\begin{lemma}
  \label{lem:op-simple}
  Let $I,J\subseteq\atoms$.
  Then $T_{\PP^s}\left(I^h\cup J^b\right) = \left(T_\PP(J)\right)^h\cup I^b$.
\end{lemma}
\begin{proof}
  \begin{align*}
    T_{\PP^s}\left(I^h\cup J^b\right) &= \{\head(r)\mid\body(r)^{I^h\cup J^b}=\ltrue\} \\
    &= \{\head(r)^h\mid(\body(r)^b)^{I^h\cup J^b}=\ltrue\}\cup\{a^h\mid a^b\in I^h\cup J^b\} \\
    &= \left(T_\PP(J)\right)^h\cup I^b
  \end{align*}
  by unfolding the relevant definitions.
\end{proof}
Observe that any set in $\atoms^h\cup\atoms^b$ can be written in the form stated in the hypothesis of this lemma.

\begin{proposition}
  \label{prop:simple-fp}
  If $I^h\cup J^b$ is a fixpoint of $T_{\PP^s}$, then $I=J$ and $I$ is a fixpoint of $T_\PP$.
\end{proposition}
\begin{proof}
  If $I^h\cup J^b=T_{\PP^s}\left(I^h\cup J^b\right)=T_\PP(J)^h\cup I^b$, then necessarily $I^h=T_\PP(J)^h$ and $J^b=I^b$.
  From the latter equality we conclude that $I=J$, whence the former yields $I=T_\PP(J)=T_\PP(I)$.
\end{proof}

\begin{proposition}
  \label{prop:simple-gf}
  $I$ is a grounded fixpoint of $T_\PP$ iff $I^h\cup I^b$ is a grounded fixpoint of $T_{\PP^s}$.
\end{proposition}
\begin{proof}
  Since grounded and strictly grounded fixpoints coincide in this setting, we show the result for strictly grounded fixpoints.

  Suppose that $I^h\cup I^b$ is a grounded fixpoint of $T_{\PP^s}$.
  By the previous lemma, $I$ is a fixpoint of $T_\PP$.
  Let $J\subsetneq I$ be such that $T_\PP(J)\cap I\subseteq J$.
  Using Lemma~\ref{lem:op-simple}, we have that
  \begin{align*}
    T_{\PP^s}\left(J^h\cup J^b\right)\cap\left(I^h\cup I^b\right) &=
    \left(T_\PP(J)^h\cup J^b\right)\cap\left(I^h\cup I^b\right) \\
    &= (\underbrace{T_\PP(J)^h\cap I^h}_{\subseteq J^h})\cup(\underbrace{J^b\cap I^b}_{\subseteq J^b}) \\
    \subseteq J^h\cup J^b
  \end{align*}
  contradicting the fact that $I^h\cup I^b$ is a grounded fixpoint of $T_{\PP^s}$.

  Conversely, suppose that $I$ is a grounded fixpoint of $T_\PP$ and consider an arbitrary strict subset $J_1^h\cup J_2^b$ of $I^h\cup I^b$.
  We already established that $I^h\cup I^b$ is a fixpoint of $T_{\PP^s}$.
  Again using Lemma~\ref{lem:op-simple}, we also have that
  \begin{align*}
    T_{\PP^s}\left(J_1^h\cup J_2^b\right)\cap\left(I^h\cup I^b\right) &=
    \left(T_\PP(J_2)^h\cup J_1^b\right)\cap\left(I^h\cup I^b\right) \\
    &= (T_\PP(J_2)^h\cap I^h)\cup(J_1^b\cap I^b)
  \end{align*}
  If this is a subset of $J_1^h\cup J_2^b$, then we conclude that $T_\PP(J_2)^h\cap I^h\subseteq J_1^h$ and $J_1^b\cap I^b\subseteq J_2^b$, which imply that
  $T_\PP(J_2)\cap I\subseteq J_1$ and $J_1\cap I\subseteq J_2$.
  Then $T_\PP(J_2)\cap I\subseteq J_1\cap I\subseteq J_2$.
  Furthermore, if $J_2=I$, then the fact that $T_\PP(I)=I$ would imply that also $J_1=I$, contradicting $J_1^h\cup J_2^b\subsetneq I^h\cup I^b$.
  Therefore $I$ is not a grounded fixpoint of $T_\PP$.
\end{proof}

Combining Propositions~\ref{prop:simple-fp} and~\ref{prop:simple-gf}, we obtain a characterization of all grounded fixpoints of $\PP^s$.

\begin{corollary}
  $I^h\cup J^b$ is a grounded fixpoint of $T_{\PP^s}$ iff $I=J$ and $I$ is a grounded fixpoint of $T_\PP$.
\end{corollary}

Furthermore, since $\PP^s$ can be constructed from $\PP$ in polynomial time, we also obtain a complexity result.

\begin{corollary}
  \label{cor:simple-has-gf}
  The problem of deciding whether a simple logic program has a grounded fixpoint is $\Sigma^P_2$-complete.
\end{corollary}


\subsection{Complexity of AFT-style semantics for AICs}

% \todo{streamline section}


We begin this section by stating an observation about the complexity of computing \Ap.
All complexity results are in terms of the size of the database, \fulldb.
% In the entire section, we assume $\aics$ and \atoms are finite. 
\begin{proposition}
  \label{prop:At-poly}
  Given a partial action set \UUU, 
  $\Ap(\UUU)$ is computable in polynomial time. % on the size of 
\end{proposition}
\begin{proof}
    The definition of \Ap only requires evaluating $\UUU(a)$, $\suppin_{\UUU}(a)$ and $\suppout_{\UUU}(a)$ for each $a\in\atoms$.
    In turn, the two last computations can be done in polynomial time: they require evaluating each literal in the body of each rule from $\eta$ with head $\add a$ or $\remove a$ and computing its truth value under the database updated by $\UUU$.
\end{proof}


% As a consequence, we get the following complexity results.
\begin{proposition}
  The Kripke-Kleene repair for $\fulldb$ is computable in polynomial time.
  \label{prop:KK-poly}
\end{proposition}
\begin{proof}
  The Kripke-Kleene repair of $\fulldb$ can be computed by iterating $\Ap$ until a fixpoint is reached.
  Since $\Ap$ is monotonic, the maximum number of iterations is the size of $\atoms$; since each iteration can be computed in polynomial time (Proposition~\ref{prop:At-poly}), so can this fixpoint.
\end{proof}

\begin{proposition}\label{prop:compl:wf}
  The ATF-well-founded repair for $\fulldb$ is computable in polynomial time.
\end{proposition}

The proof makes use of the following proposition. 
\begin{proposition}[\citeauthor{lpnmr/DeneckerV07},~\citeyear{lpnmr/DeneckerV07}]\label{prop:biggestufs}
Let $A$ be an approximator of $O$ and $(x,y)\in L^2$. 
Let $S_A^x$ be the operator on $L$ that maps every $y'$ to $A(x,y')_2$.
This operator is monotone. 
The smallest $y'$ such that $(x,y')$ is an unfoundedness refinement of $(x,y)$ is given by 
$y'=\lfp (S_A^x)$.
 \end{proposition}
 \begin{proof}[Proof of Proposition \ref{prop:compl:wf}]
 To compute the \Ap-well-founded fixpoint, we can construct a well-founded induction with only strict refinements. 
 Since such a well-founded is $\leqp$-increasing, it can consist of at most of $|\atoms|$ steps. 
 Computing if there exists a strict application refinement of a given partial repair set \UUU
can be done by computing $\Ap(\UUU)$. Now, Proposition \ref{prop:biggestufs} shows that the most precise unfoundedness refinement can also be computed as the least fixpoint of a derived operator on $2^\atoms$. Such a fixpoint can again be computed in polynomial time. Hence, it follows that we can compute a terminal well-founded induction, and thus the well-founded fixpoint, in polynomial time. 
%  
%   \luis{not sure about this one: Bart writes ``a well-founded induction runs in polytime in the height of the lattice if computing $A(x,y)$ can be done in polytime''.  So it should follow from Proposition~\ref{prop:At-poly}, but I'm missing the argument (I only get NP).}
\end{proof}

\begin{proposition}\label{prop:stable-complexity}
  The task of checking if a database $\fulldb$ has a stable repair is NP-complete.
\end{proposition}
\begin{proof}
  \emph{(Inclusion)} Given a candidate repair, checking that it is stable can be done in polynomial time, as it amounts to verifying that it is a repair (two-valued) and that it is a least fixpoint of the operators $\Ap(\cdot,y)_1$ and $\Ap(x,\cdot)_2$.
  The latter can be done in polynomial time, as in the proof of Proposition~\ref{prop:KK-poly}.

  \emph{(Hardness)} For completeness, we use a reduction from simple logic programs to AICs, as defined in~\cite{tplp/CaropreseT11}.
  A simple logic program is one where rules are not trivial tautologies, i.e., they do not contain contradictory atoms in their bodies and they do not include their head in their body.
  The operator $\aicop$ from~\cite{tplp/CaropreseT11} is defined as follows: if $r$ is the logic programming rule $a\leftarrow \ell_1,\ldots,\ell_n$, then $\aicop(r)$ is ${+a}\leftarrow \ell_1\wedge\ldots\wedge\ell_n\wedge\neg a$.
  Given a logic program $P$, we define $\aicop(P)=\bigcup\{\aicop(r)\mid r\in P\}$.

  This operator preserves stable semantics by Theorem \ref{thm:partialstable-LP}, and therefore allows us to compute stable models of simple logic programs by first translating them (in linear time) to sets of AICs.
  Since checking whether a logic program has a stable model is NP-complete, we conclude that checking whether a database has a stable repair must be NP-hard.
  Note that every logic program can be transformed in a simple logic program by removing the offending rules without changing its stable semantics.
\end{proof}

\begin{proposition}
  \label{prop:grounded-complexity}
  Let $\db$ be a database and $\eta$ be a set of normal AICs over $\db$.
  The problem of deciding whether there exists a  grounded repair for $\fulldb$ is $\Sigma^P_2$-complete.
\end{proposition}
\begin{proof}
  \emph{(Inclusion)} We need to show that we can decide the problem with a non-deterministic Turing machine with an NP oracle.
  Given a set of update actions $\UU$, checking that it is a fixpoint of $\Op$ can be done in polynomial time on the size of $\db$ and $\eta$; the NP-oracle can then answer whether there exists $\UU'\subsetneq\UU$ with $\Op(\UU')\cap\UU\subseteq\UU'$, thereby establishing whether $\UU$ is grounded.

  \emph{(Hardness)}
  The proof is similar to the corresponding part of the proof of Proposition~\ref{prop:stable-complexity}, using the fact that deciding whether a simple logic program has a grounded fixpoint is $\Sigma^P_2$-complete (Corollary~\ref{cor:simple-has-gf}).
  %% We invoke the (polynomial time) translation $\mathit{aic}$ from logic programs to sets of AICs over the empty database given Section~7 of \citet{tplp/CaropreseT11}.
  %% Given a logic program $\mathcal P$, deciding whether $\langle\emptyset,\mathit{aic}(\mathcal P)\rangle$ has a grounded repair is equivalent to deciding whether $\mathcal P$ has a grounded model, which is $\Sigma^P_2$-complete by Theorem~5.7 of \mycitet{GroundedFixpoints}. \bart{We should prove (not just claim) that these grounded repairs are equal to grounded models. Should be doable with what we have from the previous section}
  %% \bart{In fact, we have an example that this is NOT the case, so something definitely is wrong here. Wat we  CAN do is take the logic program from the grounded fixpoints paper, introduce some auxiliary variables (a $z_i$ that is a copy of $y_i$ and a $q'$ that is a copy of $q$) to make it ``simple'' and the proof will still hold. However, not so nice. }
\end{proof}


What we notice in this section is that complexity for inference tasks related to our semantics is always the same as the complexity of its counterpart in (normal) logic programming. 
This illustrates that the added expressivity (essentially, allowing AICs that are not unipolar) does not result in added complexity. 

%% \bart{vague... What do you mean here? }
%% \luis{In the original work, AICs are first-order things that represent the set of all their closed instances. This can give an exponential reduction in the size of $\eta$. I'm trying to say that the complexity still remains the same, because you don't need to compute that set. Maybe we can just ignore this point.}
%% \bart{Ah, I didn't read that. First order things with simple bodies (no arbitrary FO formulas, but universally quantified rules I guess. We can say something about that, but need to be careful: our complexity results change to DATA complexity results than, since by blowing up the number of variables, we can get more exponential behaviour}
%% \bart{If we say this, at the end or the beginning of the section (holds for all results of this section)}
%% \luis{Moved it to the end. The sentence still needs to be fixed.}


Contrary to the original work introducing AICs \cite{ppdp/FlescaGZ04}, our definitions do not include first-order quantifications. 
When allowing such a richer syntax, the results presented in this section can be re-used and constitute data-complexity results. 
% These result still holds if we allow a truly first-order syntax for AICs, where the atoms can include variables that are implictly universally quantified.





%% \bart{Lower bound for stable: every normal logic program can be seen  as an AIC. Should have te same stable smeantics (note: our transformation in theprevious section adds rules 
%% \[a\lrule a\] but these do not change stable models. 

%% Lower bounds for KK well-founded. Well...polytime upper bound is al that needs to be sshown?}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../AFT-semantics-AIC.tex"
%%% End:
